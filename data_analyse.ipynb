{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_column_names(column_names,size = 4):\n",
    "    '''\n",
    "    when there are too many columns\n",
    "    display every four column names in a line\n",
    "    '''\n",
    "    for i in range(1,len(column_names),size):\n",
    "        print(column_names[i:i+4].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pos' 'Ref' 'Anc' 'Alt']\n",
      "['Type' 'Length' 'isTv' 'isDerived']\n",
      "['AnnoType' 'Consequence' 'ConsScore' 'ConsDetail']\n",
      "['GC' 'CpG' 'mapAbility20bp' 'mapAbility35bp']\n",
      "['scoreSegDup' 'priPhCons' 'mamPhCons' 'verPhCons']\n",
      "['priPhyloP' 'mamPhyloP' 'verPhyloP' 'GerpN']\n",
      "['GerpS' 'GerpRS' 'GerpRSpval' 'bStatistic']\n",
      "['mutIndex' 'dnaHelT' 'dnaMGW' 'dnaProT']\n",
      "['dnaRoll' 'mirSVR-Score' 'mirSVR-E' 'mirSVR-Aln']\n",
      "['targetScan' 'fitCons' 'cHmmTssA' 'cHmmTssAFlnk']\n",
      "['cHmmTxFlnk' 'cHmmTx' 'cHmmTxWk' 'cHmmEnhG']\n",
      "['cHmmEnh' 'cHmmZnfRpts' 'cHmmHet' 'cHmmTssBiv']\n",
      "['cHmmBivFlnk' 'cHmmEnhBiv' 'cHmmReprPC' 'cHmmReprPCWk']\n",
      "['cHmmQuies' 'EncExp' 'EncH3K27Ac' 'EncH3K4Me1']\n",
      "['EncH3K4Me3' 'EncNucleo' 'EncOCC' 'EncOCCombPVal']\n",
      "['EncOCDNasePVal' 'EncOCFairePVal' 'EncOCpolIIPVal' 'EncOCctcfPVal']\n",
      "['EncOCmycPVal' 'EncOCDNaseSig' 'EncOCFaireSig' 'EncOCpolIISig']\n",
      "['EncOCctcfSig' 'EncOCmycSig' 'Segway' 'tOverlapMotifs']\n",
      "['motifDist' 'motifECount' 'motifEName' 'motifEHIPos']\n",
      "['motifEScoreChng' 'TFBS' 'TFBSPeaks' 'TFBSPeaksMax']\n",
      "['isKnownVariant' 'ESP_AF' 'ESP_AFR' 'ESP_EUR']\n",
      "['TG_AF' 'TG_ASN' 'TG_AMR' 'TG_AFR']\n",
      "['TG_EUR' 'minDistTSS' 'minDistTSE' 'GeneID']\n",
      "['FeatureID' 'CCDS' 'GeneName' 'cDNApos']\n",
      "['relcDNApos' 'CDSpos' 'relCDSpos' 'protPos']\n",
      "['relProtPos' 'Domain' 'Dst2Splice' 'Dst2SplType']\n",
      "['Exon' 'Intron' 'oAA' 'nAA']\n",
      "['Grantham' 'PolyPhenCat' 'PolyPhenVal' 'SIFTcat']\n",
      "['SIFTval' 'RawScore' 'PHRED' 'chr_pos']\n",
      "['INFO']\n"
     ]
    }
   ],
   "source": [
    "display_column_names(datatable_pd.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted columns that I do not know how to impute:\n",
      "3 EncH3K27Ac\n",
      "78 EncNucleo\n",
      "17 cDNApos\n",
      "17 relcDNApos\n",
      "16 CDSpos\n",
      "16 relCDSpos\n",
      "16 protPos\n",
      "16 relProtPos\n",
      "643 Dst2Splice\n",
      "83 Grantham\n",
      "Saved to data/dummy_no_nan_data.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    # read data file\n",
    "    datafile = os.path.join('data','mhy7.tsv')\n",
    "    datatable_pd = pd.read_csv(datafile,sep='\\t')\n",
    "    datatable_pos = datatable_pd['Pos']\n",
    "    \n",
    "    # delete some columns that were not used in cadd paper\n",
    "    del_cols = ['#Chrom','Pos','isDerived','AnnoType','ConsScore',\n",
    "                'ConsDetail','mapAbility20bp','mapAbility35bp',\n",
    "                'scoreSegDup','isKnownVariant','ESP_AF','ESP_AFR',\n",
    "                'ESP_EUR','TG_AF','TG_ASN','TG_AMR','TG_AFR','TG_EUR',\n",
    "                'GeneID','FeatureID','CCDS','GeneName','Exon',\n",
    "                'Intron','RawScore']\n",
    "    datatable_pd = datatable_pd.drop(columns=del_cols)\n",
    "\n",
    "    # delete columns without a single value\n",
    "    datatable_pd = datatable_pd.dropna(axis=1,how='all')\n",
    "\n",
    "    # fill in values recommended by cadd paper\n",
    "    values = {'GerpRS':0, 'GerpRSpval':1,'EncExp':0,'EncOCC':5,\n",
    "              'EncOCCombPVal':0,'EncOCDNasePVal':0,'EncOCFairePVal':0,\n",
    "              'EncOCpolIIPVal':0,'EncOCctcfPVal':0,'EncOCmycPVal':0,\n",
    "              'EncOCDNaseSig':0,'EncOCFaireSig':0,'EncOCpolIISig':0,\n",
    "              'EncOCctcfSig':0,'EncOCmycSig':0,'tOverlapMotifs':0,\n",
    "              'motifDist':0,'TFBS':0,'TFBSPeaksMax':0,'PolyPhenVal':0,\n",
    "              'SIFTval':0,'TFBSPeaks':0}\n",
    "    datatable_pd = datatable_pd.fillna(values)\n",
    "    \n",
    "    # transform objects to dummies\n",
    "    categorical_feature_names = \\\n",
    "    datatable_pd.select_dtypes(include=np.object).columns\n",
    "    categories={} # contains all the levels in those feature columns\n",
    "    for f in categorical_feature_names:\n",
    "        datatable_pd[f] = datatable_pd[f].astype('category')\n",
    "        categories[f] = datatable_pd[f].cat.categories\n",
    "\n",
    "    dummy_data = pd.get_dummies(datatable_pd,columns=[col for col in\n",
    "                                                      categorical_feature_names\n",
    "                                                      if col not in ['INFO']])\n",
    "    \n",
    "    # change info column into scalar column\n",
    "    dummy_data['INFO'] = datatable_pd['INFO'].astype('category').cat.codes\n",
    "    \n",
    "    # drop nan values -TODO\n",
    "    dummy_data_del_all_nan = dummy_data.copy()\n",
    "    print('Deleted columns that I do not know how to impute:')\n",
    "    for col in dummy_data.columns:\n",
    "        null = dummy_data[col].isnull().values.ravel().sum()\n",
    "        if null > 0:\n",
    "            print(null,col)\n",
    "            dummy_data_del_all_nan = dummy_data_del_all_nan.drop(columns=col)\n",
    "    \n",
    "    # normalized the numerical values before any processing afterwards\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    dummy_data_scaled = min_max_scaler.fit_transform(dummy_data_del_all_nan)\n",
    "    dummy_data_scaled = pd.DataFrame(dummy_data_scaled,\n",
    "                                     columns=dummy_data_del_all_nan.columns)\n",
    "\n",
    "    # save the preprocessed data as csv file\n",
    "    dummy_data_scaled['POS'] = datatable_pos\n",
    "    res_path = os.path.join('data','dummy_no_nan_data.csv')\n",
    "    dummy_data_scaled.to_csv(res_path,sep='\\t',index=False)\n",
    "    print('Saved to %s'%res_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 83)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatable_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 1002)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 992)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_data_del_all_nan.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try pca+LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.read_data import dataset,Datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# feature extractors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVC\n",
    "# finetuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_set(data_table,test_size=0.25,BENCHMARK=False):\n",
    "    '''\n",
    "    convert a pandas dataframe data table into Datasets(dataset,dataset)\n",
    "    '''\n",
    "    train, test = train_test_split(data_table,test_size=0.25)\n",
    "    train_x = train[[col for col in train.columns\n",
    "    if col not in ['INFO','gavin_res']]]\n",
    "    features = train_x.columns\n",
    "    train_x = np.array(train_x)\n",
    "    test_x = np.array(test[[col for col in train.columns\n",
    "    if col not in ['INFO','gavin_res']]])\n",
    "    train_y = np.array(train['INFO'],dtype=np.int8)\n",
    "    test_y = np.array(test['INFO'],dtype=np.int8)\n",
    "\n",
    "    # # check what columns are in the train Dataset\n",
    "    # for i in range(0,len(train_x.columns),5):\n",
    "    #     print(train_x.columns[i:i+5])\n",
    "\n",
    "    if BENCHMARK:\n",
    "        return Datasets(train=dataset(train_x,train_y,features),\n",
    "                        test=dataset(test_x,test_y,features)),\\\n",
    "                        train['gavin_res'],\\\n",
    "                        test['gavin_res']\n",
    "    return Datasets(train=dataset(train_x,train_y,features),\n",
    "                    test=dataset(test_x,test_y,features))\n",
    "\n",
    "def run_display_output(classifier,test,DRAW=False):\n",
    "    '''\n",
    "    get confusion matrix and auc score for test dataset\n",
    "    (optional) draw roc curve\n",
    "    '''\n",
    "    pred = classifier.predict(test.values)\n",
    "    tn, fp, fn, tp = confusion_matrix(test.labels,pred).ravel()#confusion matrix\n",
    "    print(tn,fp,fn,tp)\n",
    "    sensitivity = tp/(fn+tp)\n",
    "    specificity = tn/(fp+tn)\n",
    "    prods = classifier.predict_proba(test.values)[:,1]\n",
    "    fpr, tpr, _ = metrics.roc_curve(test.labels, prods)\n",
    "    score = metrics.auc(fpr,tpr) #auc score\n",
    "    if DRAW:\n",
    "        draw_roc_curve(fpr,tpr,score)\n",
    "\n",
    "    return sensitivity, specificity, score\n",
    "\n",
    "def display_res_gavin_and_best_model(param_grid,pipeline,mvid,filename=None):\n",
    "    '''\n",
    "    use model defined by pipeline to fit mvid Dataset\n",
    "    gridsearchCV determine the parameters given in param_grid\n",
    "    (optional) save the model in path given in filename\n",
    "    '''\n",
    "    classifier = GridSearchCV(estimator=pipeline,\n",
    "                              param_grid=param_grid)\n",
    "\n",
    "    print('Start training...')\n",
    "    classifier.fit(mvid.train.values,mvid.train.labels)\n",
    "    print('Model Description:\\n',classifier.best_estimator_)\n",
    "    if filename:\n",
    "        pickle.dump(classifier,open(filename,'wb'))\n",
    "        print('Saved model to path:',filename)\n",
    "    sensitivity,specificity,score = run_display_output(classifier,mvid.test)\n",
    "    print('>>> best model results: sensitivity: {:.{prec}}\\tspecificity: {:.{prec}f}\\tauc:{}'.\\\n",
    "    format(sensitivity,specificity,score,prec=3))\n",
    "    return classifier\n",
    "\n",
    "def read_gavin(gavin_res, labels):\n",
    "    '''\n",
    "    compare gavin results with labels for a certain subset of data\n",
    "    '''\n",
    "    gavin_res = gavin_res.replace('Pathogenic',1)\n",
    "    gavin_res = gavin_res.replace('Benign',0)\n",
    "    tn_g, fp_g, fn_g, tp_g = \\\n",
    "    confusion_matrix(labels, gavin_res.astype(np.int8)).ravel()\n",
    "    sensitivity_g = tp_g/(fn_g+tp_g)\n",
    "    specificity_g = tn_g/(fp_g+tn_g)\n",
    "    return sensitivity_g, specificity_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. (637, 991)\n",
      "Start training...\n",
      "Model Description:\n",
      " Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=10, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('logr', LogisticRegression(C=2, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
      "56 15 45 97\n",
      ">>> best model results: sensitivity: 0.683\tspecificity: 0.789\tauc:0.8090656615750844\n",
      ">>> gavin model results: sensitivity: 0.739\tspecificity: 0.070\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "\n",
    "    # read data\n",
    "    data = pd.read_csv('data/dummy_no_nan_data_with_gavinres.tsv',sep='\\t')\n",
    "    data = data.drop('POS',axis=1) # drop pos\n",
    "    mvid, train_gavin, test_gavin = read_data_set(data,BENCHMARK=True)\n",
    "    # print(data.head())\n",
    "    # raise NotImplementedError # check the dataset loaded\n",
    "    print('Dataset loaded.',mvid.train.values.shape)\n",
    "\n",
    "# ================model selection==========================================\n",
    "    # # PCA + LogisticRegression\n",
    "    # # Parameters\n",
    "    n_components = [10]#np.arange(10,100,10)\n",
    "    class_weight = ['balanced']#,{1:4,0:1},{1:2,0:1}]\n",
    "    param_grid_logr = [{'pca__n_components':n_components,\n",
    "                   'logr__penalty':['l1'],#'l2'],\n",
    "                   'logr__C':[2],#,3,4,5],\n",
    "                   'logr__class_weight':class_weight}]\n",
    "    # pipeline\n",
    "    pipeline_logr = Pipeline(steps=[('pca',PCA()),\n",
    "                               ('logr',LogisticRegression())])\n",
    "    # save model\n",
    "    filename = os.path.join('model')#,'pca_logr_new.sav')\n",
    "    # display results\n",
    "    classifier_logr = display_res_gavin_and_best_model(param_grid_logr,\n",
    "                                     pipeline_logr,\n",
    "                                     mvid)#,\n",
    "                                     #filename)\n",
    "    # display gavin results\n",
    "    sensitivity_g,specificity_g = read_gavin(test_gavin,mvid.test.labels)\n",
    "    print('>>> gavin model results: sensitivity: {:.{prec}}\\tspecificity: {:.{prec}f}'.\\\n",
    "    format(sensitivity_g,specificity_g,prec=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.11853035,  0.0780476 ,  0.06277689,  0.04903768,  0.04624863,\n",
       "        0.04406613,  0.03369834,  0.02918563,  0.02725033,  0.02369261])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_logr.best_estimator_.steps[0][1].explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
