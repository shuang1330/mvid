{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_column_names(column_names,size = 4):\n",
    "    '''\n",
    "    when there are too many columns\n",
    "    display every four column names in a line\n",
    "    '''\n",
    "    for i in range(1,len(column_names),size):\n",
    "        print(column_names[i:i+4].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted columns that I do not know how to impute:\n",
      "3 EncH3K27Ac\n",
      "78 EncNucleo\n",
      "17 cDNApos\n",
      "17 relcDNApos\n",
      "16 CDSpos\n",
      "16 relCDSpos\n",
      "16 protPos\n",
      "16 relProtPos\n",
      "643 Dst2Splice\n",
      "83 Grantham\n",
      "Saved to data/dummy_no_nan_data.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    # read data file\n",
    "    datafile = os.path.join('data','mhy7.tsv')\n",
    "    datatable_pd = pd.read_csv(datafile,sep='\\t')\n",
    "    datatable_pos = datatable_pd['Pos']\n",
    "    \n",
    "    # delete some columns that were not used in cadd paper\n",
    "    del_cols = ['#Chrom','Pos','isDerived','AnnoType','ConsScore',\n",
    "                'ConsDetail','mapAbility20bp','mapAbility35bp',\n",
    "                'scoreSegDup','isKnownVariant','ESP_AF','ESP_AFR',\n",
    "                'ESP_EUR','TG_AF','TG_ASN','TG_AMR','TG_AFR','TG_EUR',\n",
    "                'GeneID','FeatureID','CCDS','GeneName','Exon',\n",
    "                'Intron','RawScore']\n",
    "    datatable_pd = datatable_pd.drop(columns=del_cols)\n",
    "\n",
    "    # delete columns without a single value\n",
    "    datatable_pd = datatable_pd.dropna(axis=1,how='all')\n",
    "\n",
    "    # fill in values recommended by cadd paper\n",
    "    values = {'GerpRS':0, 'GerpRSpval':1,'EncExp':0,'EncOCC':5,\n",
    "              'EncOCCombPVal':0,'EncOCDNasePVal':0,'EncOCFairePVal':0,\n",
    "              'EncOCpolIIPVal':0,'EncOCctcfPVal':0,'EncOCmycPVal':0,\n",
    "              'EncOCDNaseSig':0,'EncOCFaireSig':0,'EncOCpolIISig':0,\n",
    "              'EncOCctcfSig':0,'EncOCmycSig':0,'tOverlapMotifs':0,\n",
    "              'motifDist':0,'TFBS':0,'TFBSPeaksMax':0,'PolyPhenVal':0,\n",
    "              'SIFTval':0,'TFBSPeaks':0}\n",
    "    datatable_pd = datatable_pd.fillna(values)\n",
    "    \n",
    "    # transform objects to dummies\n",
    "    categorical_feature_names = \\\n",
    "    datatable_pd.select_dtypes(include=np.object).columns\n",
    "    categories={} # contains all the levels in those feature columns\n",
    "    for f in categorical_feature_names:\n",
    "        datatable_pd[f] = datatable_pd[f].astype('category')\n",
    "        categories[f] = datatable_pd[f].cat.categories\n",
    "\n",
    "    dummy_data = pd.get_dummies(datatable_pd,columns=[col for col in\n",
    "                                                      categorical_feature_names\n",
    "                                                      if col not in ['INFO']])\n",
    "    \n",
    "    # change info column into scalar column\n",
    "    dummy_data['INFO'] = datatable_pd['INFO'].astype('category').cat.codes\n",
    "    \n",
    "    # drop nan values -TODO\n",
    "    dummy_data_del_all_nan = dummy_data.copy()\n",
    "    print('Deleted columns that I do not know how to impute:')\n",
    "    for col in dummy_data.columns:\n",
    "        null = dummy_data[col].isnull().values.ravel().sum()\n",
    "        if null > 0:\n",
    "            print(null,col)\n",
    "            dummy_data_del_all_nan = dummy_data_del_all_nan.drop(columns=col)\n",
    "    \n",
    "    # normalized the numerical values before any processing afterwards\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    dummy_data_scaled = min_max_scaler.fit_transform(dummy_data_del_all_nan)\n",
    "    dummy_data_scaled = pd.DataFrame(dummy_data_scaled,\n",
    "                                     columns=dummy_data_del_all_nan.columns)\n",
    "\n",
    "    # save the preprocessed data as csv file\n",
    "    dummy_data_scaled['POS'] = datatable_pos\n",
    "    res_path = os.path.join('data','dummy_no_nan_data.csv')\n",
    "    dummy_data_scaled.to_csv(res_path,sep='\\t',index=False)\n",
    "    print('Saved to %s'%res_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 83)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatable_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 1002)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 992)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_data_del_all_nan.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try pca+LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.read_data import dataset,Datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# feature extractors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVC\n",
    "# finetuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_set(data_table,test_size=0.25,BENCHMARK=False):\n",
    "    '''\n",
    "    convert a pandas dataframe data table into Datasets(dataset,dataset)\n",
    "    '''\n",
    "    train, test = train_test_split(data_table,test_size=0.25)\n",
    "    train_x = train[[col for col in train.columns\n",
    "    if col not in ['INFO','gavin_res']]]\n",
    "    features = train_x.columns\n",
    "    train_x = np.array(train_x)\n",
    "    test_x = np.array(test[[col for col in train.columns\n",
    "    if col not in ['INFO','gavin_res']]])\n",
    "    train_y = np.array(train['INFO'],dtype=np.int8)\n",
    "    test_y = np.array(test['INFO'],dtype=np.int8)\n",
    "\n",
    "    # # check what columns are in the train Dataset\n",
    "    # for i in range(0,len(train_x.columns),5):\n",
    "    #     print(train_x.columns[i:i+5])\n",
    "\n",
    "    if BENCHMARK:\n",
    "        return Datasets(train=dataset(train_x,train_y,features),\n",
    "                        test=dataset(test_x,test_y,features)),\\\n",
    "                        train['gavin_res'],\\\n",
    "                        test['gavin_res']\n",
    "    return Datasets(train=dataset(train_x,train_y,features),\n",
    "                    test=dataset(test_x,test_y,features))\n",
    "\n",
    "def run_display_output(classifier,test,DRAW=False):\n",
    "    '''\n",
    "    get confusion matrix and auc score for test dataset\n",
    "    (optional) draw roc curve\n",
    "    '''\n",
    "    pred = classifier.predict(test.values)\n",
    "    tn, fp, fn, tp = confusion_matrix(test.labels,pred).ravel()#confusion matrix\n",
    "    print(tn,fp,fn,tp)\n",
    "    sensitivity = tp/(fn+tp)\n",
    "    specificity = tn/(fp+tn)\n",
    "    prods = classifier.predict_proba(test.values)[:,1]\n",
    "    fpr, tpr, _ = metrics.roc_curve(test.labels, prods)\n",
    "    score = metrics.auc(fpr,tpr) #auc score\n",
    "    if DRAW:\n",
    "        draw_roc_curve(fpr,tpr,score)\n",
    "\n",
    "    return sensitivity, specificity, score\n",
    "\n",
    "def display_res_gavin_and_best_model(param_grid,pipeline,mvid,filename=None):\n",
    "    '''\n",
    "    use model defined by pipeline to fit mvid Dataset\n",
    "    gridsearchCV determine the parameters given in param_grid\n",
    "    (optional) save the model in path given in filename\n",
    "    '''\n",
    "    classifier = GridSearchCV(estimator=pipeline,\n",
    "                              param_grid=param_grid)\n",
    "\n",
    "    print('Start training...')\n",
    "    classifier.fit(mvid.train.values,mvid.train.labels)\n",
    "    print('Model Description:\\n',classifier.best_estimator_)\n",
    "    if filename:\n",
    "        pickle.dump(classifier,open(filename,'wb'))\n",
    "        print('Saved model to path:',filename)\n",
    "    sensitivity,specificity,score = run_display_output(classifier,mvid.test)\n",
    "    print('>>> best model results: sensitivity: {:.{prec}}\\tspecificity: {:.{prec}f}\\tauc:{}'.\\\n",
    "    format(sensitivity,specificity,score,prec=3))\n",
    "    return classifier\n",
    "\n",
    "def read_gavin(gavin_res, labels):\n",
    "    '''\n",
    "    compare gavin results with labels for a certain subset of data\n",
    "    '''\n",
    "    gavin_res = gavin_res.replace('Pathogenic',1)\n",
    "    gavin_res = gavin_res.replace('Benign',0)\n",
    "    tn_g, fp_g, fn_g, tp_g = \\\n",
    "    confusion_matrix(labels, gavin_res.astype(np.int8)).ravel()\n",
    "    sensitivity_g = tp_g/(fn_g+tp_g)\n",
    "    specificity_g = tn_g/(fp_g+tn_g)\n",
    "    return sensitivity_g, specificity_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. (637, 991)\n",
      "Start training...\n",
      "Model Description:\n",
      " Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=20, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('logr', LogisticRegression(C=2, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
      "42 27 36 108\n",
      ">>> best model results: sensitivity: 0.75\tspecificity: 0.609\tauc:0.7289653784219\n",
      ">>> gavin model results: sensitivity: 0.715\tspecificity: 0.145\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "\n",
    "    # read data\n",
    "    data = pd.read_csv('data/dummy_no_nan_data_with_gavinres.tsv',sep='\\t')\n",
    "    data = data.drop('POS',axis=1) # drop pos\n",
    "    mvid, train_gavin, test_gavin = read_data_set(data,BENCHMARK=True)\n",
    "    # print(data.head())\n",
    "    # raise NotImplementedError # check the dataset loaded\n",
    "    print('Dataset loaded.',mvid.train.values.shape)\n",
    "\n",
    "# ================model selection==========================================\n",
    "    # # PCA + LogisticRegression\n",
    "    # # Parameters\n",
    "    n_components = np.arange(10,50,10)\n",
    "    class_weight = ['balanced']#,{1:2,0:1}]\n",
    "    param_grid_logr = [{'pca__n_components':n_components,\n",
    "                   'logr__penalty':['l1'],#'l2'],\n",
    "                   'logr__C':[2],#3,4,5],\n",
    "                   'logr__class_weight':class_weight}]\n",
    "    # pipeline\n",
    "    pipeline_logr = Pipeline(steps=[('pca',PCA()),\n",
    "                               ('logr',LogisticRegression())])\n",
    "    # save model\n",
    "    filename = os.path.join('model')#,'pca_logr_new.sav')\n",
    "    # display results\n",
    "    classifier_logr = display_res_gavin_and_best_model(param_grid_logr,\n",
    "                                     pipeline_logr,\n",
    "                                     mvid)#,\n",
    "                                     #filename)\n",
    "    # display gavin results\n",
    "    sensitivity_g,specificity_g = read_gavin(test_gavin,mvid.test.labels)\n",
    "    print('>>> gavin model results: sensitivity: {:.{prec}}\\tspecificity: {:.{prec}f}'.\\\n",
    "    format(sensitivity_g,specificity_g,prec=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.11632208,  0.07721978,  0.06436453,  0.04747822,  0.04632355,\n",
       "        0.0453436 ,  0.03437331,  0.02967632,  0.02753708,  0.02352178,\n",
       "        0.02182379,  0.02064643,  0.01853344,  0.01694133,  0.0151945 ,\n",
       "        0.01455347,  0.01408986,  0.01290098,  0.01215584,  0.01103914])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_logr.best_estimator_.steps[0][1].explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use new model to test myo5b dataset\n",
    "myo5b_data = pd.read_csv('data/processed_myo5b.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "myo5b = read_data_set(myo5b_data,BENCHMARK=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 993)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(186, 180)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myo5b.train.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213, 991)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvid.test.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (62,180) (991,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f17b731dfe6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msensitivity_myo5b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspecificity_myo5b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscore_myo5b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_display_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier_logr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmyo5b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-8a9f749ded8f>\u001b[0m in \u001b[0;36mrun_display_output\u001b[0;34m(classifier, test, DRAW)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0moptional\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mdraw\u001b[0m \u001b[0mroc\u001b[0m \u001b[0mcurve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     '''\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/py3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/py3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[1;32m    466\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_estimator_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/py3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/py3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/py3/lib/python3.6/site-packages/sklearn/decomposition/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mX_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhiten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (62,180) (991,) "
     ]
    }
   ],
   "source": [
    "sensitivity_myo5b,specificity_myo5b,score_myo5b = run_display_output(classifier_logr,myo5b.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
